
# MIOpen Porting Guide

<!-- line break <br/><br/> -->

<div style="page-break-after: always"></div>

## Key differences between MIOpen v1.0 and cuDNN:
* MIOpen only supports 4-D tensors in the NCHW storage format. This means all the __“\*Nd\*”__ APIs in cuDNN do not have a corresponding API in MIOpen.
* MIOpen only supports __`float(fp32)`__ data-type.
* MIOpen only supports __2D Convolutions__ and __2D Pooling__.
* Calling miopenFindConvolution*Algorithm() is mandatory before calling any Convolution API.
* Typical calling sequence for Convolution APIs for MIOpen is:
    * miopenConvolution*GetWorkSpaceSize() // returns the workspace size required by Find()
    * miopenFindConvolution*Algorithm() // returns performance info about various algorithms
    * miopenConvolution*()
* MIOpen does not support __Preferences__ for convolutions.
* MIOpen does not support Softmax modes. MIOpen implements the __SOFTMAX_MODE_CHANNEL__ flavor.
* MIOpen does not support __Transform-Tensor__, __Dropout__, __RNNs__, and __Divisive Normalization__.

<br/><br/><br/><br/>

## Helpful MIOpen Environment Variables
`MIOPEN_ENABLE_LOGGING=1` – log all the MIOpen APIs called including the parameters passed to
those APIs. \
`MIOPEN_DEBUG_AMD_ROCM_PRECOMPILED_BINARIES=0` – disable Winograd convolution
algorithm. \
`MIOPEN_DEBUG_GCN_ASM_KERNELS=0` – disable hand-tuned asm. kernels for Direct convolution
algorithm. Fall-back to kernels written in high-level language. \
`MIOPEN_DEBUG_CONV_FFT=0` – disable FFT convolution algorithm. \
`MIOPEN_DEBUG_CONV_DIRECT=0` – disable Direct convolution algorithm.

<br/><br/><br/><br/>
<!-- Tables -->


| __Operation__ | __cnDNN API__ | __MIOpen API__|
| ------------- | ------------- | ------------- |
|| cudnnStatus_t <br>__cudnnCreate__(<br>cudnnHandle_t *handle) | miopenStatus_t <br> __miopenCreate__(<br>miopenHandle_t *handle) |
||cudnnStatus_t <br>__cudnnDestroy__(<br>cudnnHandle_t handle) |miopenStatus_t <br>__miopenDestroy__(<br>miopenHandle_t handle)|
|__Handle__|cudnnStatus_t <br>__cudnnSetStream__(<br>cudnnHandle_t handle, <br>cudaStream_t streamId)| miopenStatus_t <br>__miopenSetStream__(<br>miopenHandle_t handle, <br>miopenAcceleratorQueue_t streamID) |
||cudnnStatus_t <br>__cudnnGetStream__(<br>cudnnHandle_t handle, <br>cudaStream_t *streamId)|miopenStatus_t <br>__miopenGetStream__(<br>miopenHandle_t handle,<br> miopenAcceleratorQueue_t  *streamID)|\
|<br>|||
||cudnnStatus_t <br>__cudnnCreateTensorDescriptor__(<br>cudnnTensorDescriptor_t *tensorDesc) |miopenStatus_t <br>__miopenCreateTensorDescriptor__(<br>miopenTensorDescriptor_t  *tensorDesc)|
||cudnnStatus_t <br>__cudnnSetTensor4dDescriptor__(<br>cudnnTensorDescriptor_t tensorDesc, <br>cudnnTensorFormat_t format, <br>cudnnDataType_t dataType, <br>int n, <br>int c, <br>int h, <br>int w)|<font color="red">// Only `NCHW` format is supported</font> <br>miopenStatus_t <br>__miopenSet4dTensorDescriptor__( <br>miopenTensorDescriptor_t tensorDesc, <br>miopenDataType_t dataType, <br>int n, <br>int c, <br>int h, <br>int w)|
||cudnnStatus_t <br>__cudnnGetTensor4dDescriptor__(<br>cudnnTensorDescriptor_t tensorDesc, <br>cudnnDataType_t *dataType, <br>int *n, <br>int *c, <br>int *h, <br>int *w, <br>int *nStride, <br>int *cStride, <br>int *hStride, <br>int *wStride) |miopenStatus_t <br>__miopenGet4dTensorDescriptor__( <br>miopenTensorDescriptor_t tensorDesc, <br>miopenDataType_t *dataType, <br>int *n, <br>int *c, <br>int *h, <br>int *w, <br>int *nStride, <br>int *cStride, <br>int *hStride, <br>int *wStride) |
|__Tensor__|cudnnStatus_t <br>__cudnnDestroyTensorDescriptor__(<br>cudnnTensorDescriptor_t tensorDesc)|miopenStatus_t <br>__miopenDestroyTensorDescriptor__(<br>miopenTensorDescriptor_t tensorDesc)|
||cudnnStatus_t <br>__cudnnAddTensor__(<br>cudnnHandle_t handle, <br>const void *alpha, <br>const cudnnTensorDescriptor_t aDesc, <br>const void *A,<br> const void *beta, <br>const cudnnTensorDescriptor_t cDesc, <br>void *C)|<font color="red">//Set tensorOp to miopenOpTensorAdd</font> <br>miopenStatus_t <br>__miopenOpTensor__(<br>miopenHandle_t handle,<br> miopenTensorOp_t tensorOp, <br>const void *alpha1, <br>constmiopenTensorDescriptor_t  aDesc, <br>const void *A, <br>const void *alpha2, <br>const miopenTensorDescriptor_t bDesc, <br>const void *B,<br> const void *beta, <br>const miopenTensorDescriptor_t  cDesc, <br>void *C) <font color="red"><br>// For Forward Bias use miopenConvolutionForwardBias</font> |
||cudnnStatus_t <br>__cudnnOpTensor__(<br>cudnnHandle_t handle, <br>const cudnnOpTensorDescriptor_t opTensorDesc,<br> const void *alpha1, <br>const cudnnTensorDescriptor_t aDesc,<br> const void *A,<br> const void *alpha2, <br>const cudnnTensorDescriptor_t       bDesc, <br>const void *B, <br>const void *beta, <br>const cudnnTensorDescriptor_t cDesc, <br>void *C)|miopenStatus_t <br>__miopenOpTensor__(<br>miopenHandle_t handle, <br>miopenTensorOp_t tensorOp, <br>const void *alpha1, <br>const miopenTensorDescriptor_t aDesc, <br>const void *A, <br>const void *alpha2, <br>const miopenTensorDescriptor_t  bDesc, <br>const void *B, <br>const void *beta, <br>const miopenTensorDescriptor_t  cDesc, <br>void *C)|
||cudnnStatus_t <br>__cudnnSetTensor__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y, <br>const void *valuePtr) |miopenStatus_t <br>__miopenSetTensor__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t   yDesc, <br>void *y, <br>const void *alpha)|
||cudnnStatus_t <br>__cudnnScaleTensor__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y, <br>const void *alpha)|miopenStatus_t <br>__miopenScaleTensor__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>const void *alpha)|
|<br>|||
|__Filter__|cudnnStatus_t <br>__cudnnCreateFilterDescriptor__(<br>cudnnFilterDescriptor_t *filterDesc)|<font color="red">// All \*`FilterDescriptor`\* APIs are substituted by the respective `TensorDescriptor` APIs </font>|
|<br>|||
||cudnnStatus_t <br>__cudnnCreateConvolutionDescriptor__(<br>cudnnConvolutionDescriptor_t *convDesc)|miopenStatus_t __miopenCreateConvolutionDescriptor__(<br>miopenConvolutionDescriptor_t *convDesc)|
||cudnnStatus_t <br>__cudnnSetConvolution2dDescriptor__(<br>cudnnConvolutionDescriptor_t convDesc, <br>int pad_h, <br>int pad_w, <br>int u, <br>int v, <br>int upscalex, <br>int upscaley, <br>cudnnConvolutionMode_t mode)|miopenStatus_t <br>__miopenInitConvolutionDescriptor__(<br>miopenConvolutionDescriptor_t convDesc, <br>miopenConvolutionMode_t mode, <br>int pad_h, <br>int pad_w, <br> int u, <br>int v, <br>int upscalex, <br>int upscaley)|
||cudnnStatus_t <br>__cudnnGetConvolution2dDescriptor__(<br>const cudnnConvolutionDescriptor_t convDesc, <br>int *pad_h, <br>int *pad_y, <br>int *u, <br>int *v, <br>int *upscalex, <br>int *upscaley, <br>cudnnConvolutionMode_t *mode) |miopenStatus_t <br>__miopenGetConvolutionDescriptor__(<br>miopenConvolutionDescriptor_t convDesc, <br>miopenConvolutionMode_t *mode, <br>int *pad_h,<br>int *pad_y, <br>int *u, <br>int *v, <br>int *upscalex, <br>int *upscaley) |
||cudnnStatus_t <br>__cudnnGetConvolution2dForwardOutputDim__(<br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t inputTensorDesc, <br>const cudnnFilterDescriptor_t filterDesc, <br>int *n, <br>int *c, <br>int *h, <br>int *w)|miopenStatus_t <br>__miopenGetConvolutionForwardOutputDim__(<br>miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t inputTensorDesc, <br>const miopenTensorDescriptor_t filterDesc, <br>int *n, <br>int *c, <br>int *h, <br>int *w)|
||cudnnStatus_t <br>__cudnnDestroyConvolutionDescriptor__(<br>cudnnConvolutionDescriptor_t convDesc) |miopenStatus_t <br>__miopenDestroyConvolutionDescriptor__(<br>miopenConvolutionDescriptor_t convDesc) |
||cudnnStatus_t <br>__cudnnFindConvolutionForwardAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t       xDesc, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t yDesc, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount cudnnConvolutionFwdAlgoPerf_t *perfResults) cudnnStatus_t <br><br>__cudnnFindConvolutionForwardAlgorithmEx__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t       xDesc, <br>const void *x, <br>const cudnnFilterDescriptor_t wDesc, <br>const void *w, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount, <br>cudnnConvolutionFwdAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSizeInBytes) <br><br>cudnnStatus_t <br>__cudnnGetConvolutionForwardAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t xDesc, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t yDesc, <br>cudnnConvolutionFwdPreference_t preference, <br>size_t memoryLimitInBytes, <br>cudnnConvolutionFwdAlgo_t *algo) |<font color="red">// `FindConvolution()` is mandatory <br>// Allocate workspace prior to running this API <br>// A table with times and memory requirements for different algorithms is returned <br>// Users can choose the top-most algorithm if they only care about the fastest algorithm</font><br>miopenStatus_t <br>__miopenFindConvolutionForwardAlgorithm__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenTensorDescriptor_t wDesc, <br>const void *w, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>const int requestAlgoCount, <br>int *returnedAlgoCount, <br>miopenConvAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSize, <br>bool exhaustiveSearch) |
||cudnnStatus_t <br>__cudnnGetConvolutionForwardWorkspaceSize__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t       xDesc, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t yDesc, <br>cudnnConvolutionFwdAlgo_t algo, <br>size_t *sizeInBytes)|miopenStatus_t  <br>__miopenConvolutionForwardGetWorkSpaceSize__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t wDesc, <br>const miopenTensorDescriptor_t xDesc, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t yDesc, <br>size_t *workSpaceSize)|
|__Convolution__|cudnnStatus_t <br>__cudnnConvolutionForward__(<br>cudnnHandle_t handle, <br>const void *alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnFilterDescriptor_t wDesc, <br>const void *w, <br>const cudnnConvolutionDescriptor_t convDesc, <br>cudnnConvolutionFwdAlgo_t algo, <br>void *workSpace, <br>size_t workSpaceSizeInBytes, <br>const void *beta, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y)| miopenStatus_t <br>__miopenConvolutionForward__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenTensorDescriptor_t wDesc, <br>const void *w, <br>const miopenConvolutionDescriptor_t convDesc, <br>miopenConvFwdAlgorithm_t algo, <br>const void *beta, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>void *workSpace, <br>size_t workSpaceSize)|
||cudnnStatus_t <br>__cudnnConvolutionBackwardBias__(<br>cudnnHandle_t handle, <br>const void *alpha, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const void *beta, <br>const cudnnTensorDescriptor_t dbDesc, <br>void *db)|miopenStatus_t <br>__miopenConvolutionBackwardBias__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const void *beta, <br>const miopenTensorDescriptor_t dbDesc, <br>void *db)|
||cudnnStatus_t <br>__cudnnFindConvolutionBackwardFilterAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t xDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t  convDesc, <br>const cudnnFilterDescriptor_t dwDesc, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount, <br>cudnnConvolutionBwdFilterAlgoPerf_t *perfResults)  <br><br>cudnnStatus_t __cudnnFindConvolutionBackwardFilterAlgorithmEx__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *y, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnFilterDescriptor_t dwDesc, <br>void *dw, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount, <br>cudnnConvolutionBwdFilterAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSizeInBytes) <br><br>cudnnStatus_t <br>__cudnnGetConvolutionBackwardFilterAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t xDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnFilterDescriptor_t dwDesc, <br>cudnnConvolutionBwdFilterPreference_t preference, <br>size_t memoryLimitInBytes, <br>cudnnConvolutionBwdFilterAlgo_t *algo) |<font color="red">// `FindConvolution()` is mandatory <br>// Allocate workspace prior to running this API <br>// A table with times and memory requirements for different algorithms is returned <br>// Users can choose the top-most algorithm if they only care about the fastest algorithm</font> <br>miopenStatus_t <br>__miopenFindConvolutionBackwardWeightsAlgorithm__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t dwDesc, <br>void *dw, <br>const int requestAlgoCount, <br>int *returnedAlgoCount, <br>miopenConvAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSize, <br>bool exhaustiveSearch) |
||cudnnStatus_t <br>__cudnnGetConvolutionBackwardFilterWorkspaceSize__(<br>cudnnHandle_t handle, <br>const cudnnTensorDescriptor_t xDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnFilterDescriptor_t gradDesc, <br>cudnnConvolutionBwdFilterAlgo_t algo, <br>size_t *sizeInBytes) | miopenStatus_t <br>__miopenConvolutionBackwardWeightsGetWorkSpaceSize__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t dyDesc, <br>const miopenTensorDescriptor_t xDesc, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t dwDesc, <br>size_t *workSpaceSize)|
||cudnnStatus_t <br>__cudnnConvolutionBackwardFilter__(<br>cudnnHandle_t handle, <br>const void *alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnConvolutionDescriptor_t convDesc, <br>cudnnConvolutionBwdFilterAlgo_t algo, <br>void *workSpace, <br>size_t workSpaceSizeInBytes, <br>const void *beta, <br>const cudnnFilterDescriptor_t dwDesc, <br>void *dw)|miopenStatus_t <br>__miopenConvolutionBackwardWeights__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenConvolutionDescriptor_t convDesc, miopenConvBwdWeightsAlgorithm_t algo, <br>const void *beta, <br>const miopenTensorDescriptor_t dwDesc, <br>void *dw, <br>void *workSpace, <br>size_t workSpaceSize) |
||cudnnStatus_t <br>__cudnnGetConvolutionBackwardDataWorkspaceSize__(<br>cudnnHandle_t handle, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t dxDesc, <br>cudnnConvolutionBwdDataAlgo_t algo, <br>size_t *sizeInBytes)| miopenStatus_t <br>__miopenConvolutionBackwardDataGetWorkSpaceSize__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t dyDesc, <br>const miopenTensorDescriptor_t wDesc, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t dxDesc, <br>size_t *workSpaceSize)|
||cudnnStatus_t <br>__cudnnFindConvolutionBackwardDataAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t dxDesc, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount, <br>cudnnConvolutionBwdDataAlgoPerf_t *perfResults)<br><br>cudnnStatus_t <br>__cudnnFindConvolutionBackwardDataAlgorithmEx__(<br>cudnnHandle_t handle, <br>const cudnnFilterDescriptor_t wDesc, <br>const void *w, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx, <br>const int requestedAlgoCount, <br>int *returnedAlgoCount, <br>cudnnConvolutionBwdDataAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSizeInBytes) <br><br>cudnnStatus_t <br>__cudnnGetConvolutionBackwardDataAlgorithm__(<br>cudnnHandle_t handle, <br>const cudnnFilterDescriptor_t wDesc, <br>const cudnnTensorDescriptor_t dyDesc, <br>const cudnnConvolutionDescriptor_t convDesc, <br>const cudnnTensorDescriptor_t dxDesc, <br>cudnnConvolutionBwdDataPreference_t preference, <br>size_t memoryLimitInBytes, <br>cudnnConvolutionBwdDataAlgo_t *algo) |<font color="red">// `FindConvolution()` is mandatory <br>// Allocate workspace prior to running this API <br>// A table with times and memory requirements for different algorithms is returned <br>// Users can choose the top-most algorithm if they only care about the fastest algorithm</font> <br>miopenStatus_t __miopenFindConvolutionBackwardDataAlgorithm__(<br>miopenHandle_t handle, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t wDesc, <br>const void *w, <br>const miopenConvolutionDescriptor_t convDesc, <br>const miopenTensorDescriptor_t dxDesc, <br>const void *dx, <br>const int requestAlgoCount, <br>int *returnedAlgoCount, <br>miopenConvAlgoPerf_t *perfResults, <br>void *workSpace, <br>size_t workSpaceSize, <br>bool exhaustiveSearch)|
||cudnnStatus_t <br>__cudnnConvolutionBackwardData__(<br>cudnnHandle_t handle, <br>const void *alpha, <br>const cudnnFilterDescriptor_t wDesc, <br>const void *w, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnConvolutionDescriptor_t convDesc, <br>cudnnConvolutionBwdDataAlgo_t algo, <br>void *workSpace, <br>size_t workSpaceSizeInBytes, <br>const void *beta, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx)| miopenStatus_t <br>__miopenConvolutionBackwardData__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t wDesc, <br>const void *w, <br>const miopenConvolutionDescriptor_t convDesc, <br>miopenConvBwdDataAlgorithm_t algo, <br>const void *beta, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx, <br>void *workSpace, <br>size_t workSpaceSize)|
|<br>|||
|__Softmax__|cudnnStatus_t <br>__cudnnSoftmaxForward__(<br>cudnnHandle_t handle, <br>cudnnSoftmaxAlgorithm_t algo, <br>cudnnSoftmaxMode_t mode, <br>const void *alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y)|miopenStatus_t <br>__miopenSoftmaxForward__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t yDesc, <br>void *y)|
||cudnnStatus_t <br>__cudnnSoftmaxBackward__(<br>cudnnHandle_t handle, <br>cudnnSoftmaxAlgorithm_t algo, <br>cudnnSoftmaxMode_t mode, <br>const void *alpha, <br>const cudnnTensorDescriptor_t yDesc, <br>const void *y, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const void *beta, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx)|miopenStatus_t __miopenSoftmaxBackward__(<br>miopenHandle_t handle, <br>const void *alpha, <br>const miopenTensorDescriptor_t yDesc, <br>const void *y, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const void *beta, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx)|
|<br>|||
||cudnnStatus_t <br>__cudnnCreatePoolingDescriptor__(<br>cudnnPoolingDescriptor_t *poolingDesc)| miopenStatus_t <br>__miopenCreatePoolingDescriptor__(<br>miopenPoolingDescriptor_t *poolDesc)
||cudnnStatus_t <br>__cudnnSetPooling2dDescriptor__(<br>cudnnPoolingDescriptor_t poolingDesc, <br>cudnnPoolingMode_t mode, <br>cudnnNanPropagation_t maxpoolingNanOpt, <br>int windowHeight, <br>int windowWidth, <br>int verticalPadding, <br>int horizontalPadding, <br>int verticalStride, <br>int horizontalStride)| miopenStatus_t <br>__miopenSet2dPoolingDescriptor__(<br>miopenPoolingDescriptor_t poolDesc, <br>miopenPoolingMode_t mode, <br>int windowHeight, <br>int windowWidth, <br>int pad_h, <br>int pad_w, <br>int u, <br>int v)|
||cudnnStatus_t <br>__cudnnGetPooling2dDescriptor__(<br>const cudnnPoolingDescriptor_t poolingDesc, <br>cudnnPoolingMode_t *mode, <br>cudnnNanPropagation_t *maxpoolingNanOpt, <br>int *windowHeight, <br>int *windowWidth, <br>int *verticalPadding, <br>int *horizontalPadding, <br>int *verticalStride, <br>int *horizontalStride) | miopenStatus_t <br>__miopenGet2dPoolingDescriptor__(<br>const miopenPoolingDescriptor_t poolDesc, <br>miopenPoolingMode_t *mode, <br>int *windowHeight, <br>int *windowWidth, <br>int *pad_h, <br>int *pad_w, <br>int *u, <br>int *v)|
|__Pooling__|cudnnStatus_t <br>__cudnnGetPooling2dForwardOutputDim__(<br>const cudnnPoolingDescriptor_t poolingDesc, <br>const cudnnTensorDescriptor_t inputTensorDesc, <br>int *n, <br>int *c, <br>int *h, <br>int *w)| miopenStatus_t <br>__miopenGetPoolingForwardOutputDim__(<br>const miopenPoolingDescriptor_t poolDesc, <br>const miopenTensorDescriptor_t tensorDesc, <br>int *n, <br>int *c, <br>int *h, <br>int *w)|
||cudnnStatus_t <br>__cudnnDestroyPoolingDescriptor__(<br>cudnnPoolingDescriptor_t poolingDesc)|miopenStatus_t <br>__miopenDestroyPoolingDescriptor__(<br>miopenPoolingDescriptor_t poolDesc)
||cudnnStatus_t <br>__cudnnPoolingForward__(<br>cudnnHandle_t handle, <br>const cudnnPoolingDescriptor_t poolingDesc, <br>const void *alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y)|miopenStatus_t <br>__miopenPoolingForward__(<br>miopenHandle_t handle, <br>const miopenPoolingDescriptor_t poolDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>bool do_backward, <br>void *workSpace, <br>size_t workSpaceSize)|
|||miopenStatus_t <br>__miopenPoolingGetWorkSpaceSize__(<br>const miopenTensorDescriptor_t yDesc, <br>size_t *workSpaceSize)|
||cudnnStatus_t <br>__cudnnPoolingBackward__(<br>cudnnHandle_t handle, <br>const cudnnPoolingDescriptor_t poolingDesc, <br>const void *alpha, <br>const cudnnTensorDescriptor_t yDesc, <br>const void *y, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx)|miopenStatus_t <br>__miopenPoolingBackward__(<br>miopenHandle_t handle, <br>const miopenPoolingDescriptor_t poolDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t yDesc, <br>const void *y, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx, <br>const void *workspace)|
|<br>|||
||cudnnStatus_t <br>__cudnnCreateActivationDescriptor__(<br>cudnnActivationDescriptor_t *activationDesc)|miopenStatus_t <br>__miopenCreateActivationDescriptor__(<br>miopenActivationDescriptor_t *activDesc) |
||cudnnStatus_t <br>__cudnnSetActivationDescriptor__(<br>cudnnActivationDescriptor_t activationDesc, <br>cudnnActivationMode_t mode, <br>cudnnNanPropagation_t reluNanOpt, <br>double reluCeiling)| miopenStatus_t <br>__miopenSetActivationDescriptor__(<br>const miopenActivationDescriptor_t activDesc, <br>miopenActivationMode_t mode, <br>double activAlpha, <br>double activBeta, <br>double activPower)|
|__Activation__|cudnnStatus_t <br>__cudnnGetActivationDescriptor__(<br>const cudnnActivationDescriptor_t activationDesc, <br>cudnnActivationMode_t *mode, <br>cudnnNanPropagation_t *reluNanOpt, <br>double *reluCeiling)|miopenStatus_t <br>__miopenGetActivationDescriptor__(<br>const miopenActivationDescriptor_t activDesc, <br>miopenActivationMode_t *mode, <br>double *activAlpha, <br>double *activBeta, <br>double *activPower)|
||cudnnStatus_t <br>__cudnnDestroyActivationDescriptor__(<br>cudnnActivationDescriptor_t activationDesc)|miopenStatus_t <br>__miopenDestroyActivationDescriptor__(<br>miopenActivationDescriptor_t activDesc)|
||cudnnStatus_t <br>__cudnnActivationForward__(<br>cudnnHandle_t handle, <br>cudnnActivationDescriptor_t activationDesc, <br>const void *alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y) | miopenStatus_t <br>__miopenActivationForward__(<br>miopenHandle_t handle, <br>const miopenActivationDescriptor_t activDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t yDesc, <br>void *y) |
||cudnnStatus_t <br>__cudnnActivationBackward__(<br>cudnnHandle_t handle, <br>cudnnActivationDescriptor_t activationDesc, <br>const void *alpha, <br>const cudnnTensorDescriptor_t yDesc, <br>const void *y, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx)|miopenStatus_t <br>__miopenActivationBackward__(<br>miopenHandle_t handle, <br>const miopenActivationDescriptor_t activDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t yDesc, <br>const void *y, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx)|
|<br>|||
||cudnnStatus_t <br>__cudnnCreateLRNDescriptor__(<br>cudnnLRNDescriptor_t *normDesc)|miopenStatus_t <br>__miopenCreateLRNDescriptor__(<br>miopenLRNDescriptor_t *lrnDesc)
||cudnnStatus_t <br>__cudnnSetLRNDescriptor__(<br>cudnnLRNDescriptor_t normDesc, <br>unsigned lrnN, <br>double lrnAlpha, <br>double lrnBeta, <br>double lrnK)| miopenStatus_t __miopenSetLRNDescriptor__(<br>const miopenLRNDescriptor_t lrnDesc, <br>miopenLRNMode_t mode, <br>unsigned lrnN, <br>double lrnAlpha, <br>double lrnBeta, <br>double lrnK)
||cudnnStatus_t <br>__cudnnGetLRNDescriptor__(<br>cudnnLRNDescriptor_t normDesc, <br>unsigned* lrnN, <br>double* lrnAlpha, <br>double* lrnBeta, <br>double* lrnK) |miopenStatus_t <br>__miopenGetLRNDescriptor__(<br>const miopenLRNDescriptor_t lrnDesc, <br>miopenLRNMode_t *mode, <br>unsigned *lrnN, <br>double *lrnAlpha, <br>double *lrnBeta, <br>double *lrnK)
|__LRN__|cudnnStatus_t <br>__cudnnDestroyLRNDescriptor__(<br>cudnnLRNDescriptor_t lrnDesc)|miopenStatus_t <br>__miopenDestroyLRNDescriptor__(<br>miopenLRNDescriptor_t lrnDesc)
||cudnnStatus_t <br>__cudnnLRNCrossChannelForward__(<br>cudnnHandle_t handle, <br>cudnnLRNDescriptor_t normDesc, <br>cudnnLRNMode_t lrnMode, <br>const void* alpha, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y)|miopenStatus_t <br>__miopenLRNForward__(<br>miopenHandle_t handle, <br>const miopenLRNDescriptor_t lrnDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>bool do_backward, <br>void  *workspace)|
||cudnnStatus_t <br>__cudnnLRNCrossChannelBackward__(<br>cudnnHandle_t handle, <br>cudnnLRNDescriptor_t normDesc, <br>cudnnLRNMode_t lrnMode, <br>const void* alpha, <br>const cudnnTensorDescriptor_t yDesc, <br>const void *y, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx)| miopenStatus_t <br>__miopenLRNBackward__(<br>miopenHandle_t handle, <br>const miopenLRNDescriptor_t lrnDesc, <br>const void *alpha, <br>const miopenTensorDescriptor_t yDesc, <br>const void *y, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const void *beta, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx, <br>const void *workspace)|
|||miopenStatus_t <br>__miopenLRNGetWorkSpaceSize__(<br>const miopenTensorDescriptor_t yDesc, <br>size_t *workSpaceSize)|
|<br>|||
||cudnnStatus_t <br>__cudnnDeriveBNTensorDescriptor__(<br>cudnnTensorDescriptor_t derivedBnDesc, <br>const cudnnTensorDescriptor_t xDesc, <br>cudnnBatchNormMode_t mode)| miopenStatus_t <br>__miopenDeriveBNTensorDescriptor__(<br>miopenTensorDescriptor_t derivedBnDesc, <br>const miopenTensorDescriptor_t xDesc, <br>miopenBatchNormMode_t bn_mode)||
||cudnnStatus_t <br>__cudnnBatchNormalizationForwardTraining__(<br>cudnnHandle_t handle, <br>cudnnBatchNormMode_t mode, <br>void *alpha, <br>void *beta, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y, <br>const cudnnTensorDescriptor_t bnScaleBiasMeanVarDesc, <br>void *bnScale, <br>void *bnBias, <br>double exponentialAverageFactor, <br>void *resultRunningMean, <br>void *resultRunningVariance, <br>double epsilon, <br>void *resultSaveMean, <br>void *resultSaveInvVariance)|miopenStatus_t <br>__miopenBatchNormalizationForwardTraining__(<br>miopenHandle_t handle, <br>miopenBatchNormMode_t bn_mode, <br>void *alpha, <br>void *beta, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>const miopenTensorDescriptor_t bnScaleBiasMeanVarDesc, <br>void *bnScale, <br>void *bnBias, <br>double expAvgFactor, <br>void *resultRunningMean, <br>void *resultRunningVariance, <br>double epsilon, <br>void *resultSaveMean, <br>void *resultSaveInvVariance)
|__Batch Normalization__|cudnnStatus_t <br>__cudnnnBatchNormalizationForwardInference__(<br>cudnnHandle_t handle, <br>cudnnBatchNormMode_t mode, <br>void *alpha, <br>void *beta, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnTensorDescriptor_t yDesc, <br>void *y, <br>const cudnnTensorDescriptor_t bnScaleBiasMeanVarDesc, <br>const void *bnScale, <br>void *bnBias, <br>const void *estimatedMean, <br>const void *estimatedVariance, <br>double epsilon)| miopenStatus_t <br>__miopenBatchNormalizationForwardInference__(<br>miopenHandle_t handle, <br>miopenBatchNormMode_t bn_mode, <br>void *alpha, <br>void *beta, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenTensorDescriptor_t yDesc, <br>void *y, <br>const miopenTensorDescriptor_t bnScaleBiasMeanVarDesc, <br>void *bnScale, <br>void *bnBias, <br>void *estimatedMean, <br>void *estimatedVariance, <br>double epsilon)|
||cudnnStatus_t <br>__cudnnBatchNormalizationBackward__(<br>cudnnHandle_t handle, <br>cudnnBatchNormMode_t mode, <br>const void *alphaDataDiff, <br>const void *betaDataDiff, <br>const void *alphaParamDiff, <br>const void *betaParamDiff, <br>const cudnnTensorDescriptor_t xDesc, <br>const void *x, <br>const cudnnTensorDescriptor_t dyDesc, <br>const void *dy, <br>const cudnnTensorDescriptor_t dxDesc, <br>void *dx, <br>const cudnnTensorDescriptor_t bnScaleBiasDiffDesc, <br>const void *bnScale, <br>void *resultBnScaleDiff, <br>void *resultBnBiasDiff, <br>double epsilon, <br>const void *savedMean, <br>const void *savedInvVariance)|miopenStatus_t <br>__miopenBatchNormalizationBackward__(<br>miopenHandle_t handle, <br>miopenBatchNormMode_t bn_mode, <br>const void *alphaDataDiff, <br>const void *betaDataDiff, <br>const void *alphaParamDiff, <br>const void *betaParamDiff, <br>const miopenTensorDescriptor_t xDesc, <br>const void *x, <br>const miopenTensorDescriptor_t dyDesc, <br>const void *dy, <br>const miopenTensorDescriptor_t dxDesc, <br>void *dx, <br>const miopenTensorDescriptor_t bnScaleBiasDiffDesc, <br>const void *bnScale, <br>void *resultBnScaleDiff, <br>void *resultBnBiasDiff, <br>double epsilon, <br>const void *savedMean, <br>const void *savedInvVariance)
